import pandas as pd
from surprise import NormalPredictor
from surprise import Dataset
from surprise import Reader
from surprise import accuracy
from surprise import KNNBasic
from surprise.model_selection import cross_validate
from surprise.model_selection import train_test_split
from collections import defaultdict
import csv

'''
this function retrieves title and artist_name from the csv file
'''
def find_song_info_in_data(id, data_hash):
    result = data_hash[str(id)]
    result = str(result[0]) + ' ' + str(result[1])
    return [result]


'''
this function gets the top n recommendations generated by the algorithm
in case there are n, if not, the number will be minor
'''
def get_top_n(predictions, n=10):

    top_n = defaultdict(list)
    for uid, iid, true_r, est, _ in predictions:
        top_n[uid].append((iid, est))

    for uid, user_ratings in top_n.items():
        user_ratings.sort(key=lambda x: x[1], reverse=True)
        top_n[uid] = user_ratings[:n]

    return top_n

def main():
    row_num = 5000
    #reading the important ratings file to make it a pandas dataframe in order to be used by surprise
    ratings_data = pd.read_csv('datasets/song_dataset_ranking.txt', sep="\t", header=None, nrows = row_num)
    #define the document's columns
    ratings_data.columns = ['userId', 'songId', 'rating']
    #read the csv where it is the songs data
    song_data = open('datasets/song_data.csv', 'rt')
    c_reader = csv.reader(song_data, delimiter=',', quotechar='|')
    #create a hash where we will store the important info from all songs
    song_dict = {}
    #update the hash, example
    #keysonisonioiaofnai: ['Smoke on the water', 'Deep purple']
    for row in c_reader:
        song_dict.update({row[0]: [row[1], row[3]]})
    #surprise reader, define the rating scale to use
    reader = Reader(rating_scale=(1,100))
    #transform info to a surprise dataset
    data = Dataset.load_from_df(ratings_data, reader)
    #split data into training and testSet
    training_set, testSet = train_test_split(data, test_size=.25)
    #define the algorithm to use
    knn = KNNBasic(name="cosine", user_based=False)
    #train the algorithm
    knn.fit(training_set)
    print("Done training")
    print("Test set length", len(testSet))
    print("testing")
    #make predictions
    predictions = knn.test(testSet)
    print("getting recommendations")
    #measure accuracy, Compute FCP (Fraction of Concordant Pairs).
    accuracy.fcp(predictions)
    #get top n predictions
    top_n = get_top_n(predictions,4)
    file = open('predictions.txt', 'w')

    for uid, user_ratings in top_n.items():
        file.write("prediction for " +str(uid) +":\n")
        result_array = [find_song_info_in_data(iid,song_dict) for (iid, _) in user_ratings]
        for item in result_array:
            file.write("\t")
            file.write('-'.join(item))
            file.write("\n")
        #print("prediction for " +str(uid) +"\n" +str([find_song_info_in_data(iid,song_dict) for (iid, _) in user_ratings]) + "\n")
    file.close()





if __name__ == "__main__":
    main()
